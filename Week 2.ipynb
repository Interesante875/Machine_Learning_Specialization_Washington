{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import turicreate as tc\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('Week_2/amazon_baby_subset.csv')\n",
    "with open('Week_2/important_words.json') as f:\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    trans_dict = text.maketrans('','', string.punctuation)\n",
    "    return text.translate(trans_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2955, 198)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_perfect = products[products['perfect'] >= 1]\n",
    "contains_perfect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = dataframe[features]\n",
    "    feature_matrix = np.asmatrix(features_frame)\n",
    "    label_sarray = dataframe[label]\n",
    "    label_array = np.asmatrix(label_sarray)\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53072, 194)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, sentiment = get_numpy_data(products, important_words, 'sentiment')\n",
    "feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients \n",
    "    # YOUR CODE HERE\n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1.0/(1+np.exp(-score))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "        # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    #lp = np.sum(np.subtract(np.array((indicator-1)*scores), np.log(1. + np.exp(-scores))))\n",
    "    \n",
    "    ev = np.multiply((indicator-1.), scores)\n",
    "    nv = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    lp = np.sum(np.subtract(ev, nv))\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_1,w) using your predict_probability() function\n",
    "        # YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "\n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "\n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j]\n",
    "            # compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            # YOUR CODE HERE\n",
    "            derivative = feature_derivative(errors, feature_matrix[:, j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            # YOUR CODE HERE\n",
    "            coefficients[j] += step_size * derivative\n",
    "            \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            \n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_coefficients = np.zeros(feature_matrix.shape[1])\n",
    "step_size = 1e-7\n",
    "max_iter = 301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -36780.91768478\n",
      "iteration   1: log likelihood of observed labels = -36775.13434712\n",
      "iteration   2: log likelihood of observed labels = -36769.35713564\n",
      "iteration   3: log likelihood of observed labels = -36763.58603240\n",
      "iteration   4: log likelihood of observed labels = -36757.82101962\n",
      "iteration   5: log likelihood of observed labels = -36752.06207964\n",
      "iteration   6: log likelihood of observed labels = -36746.30919497\n",
      "iteration   7: log likelihood of observed labels = -36740.56234821\n",
      "iteration   8: log likelihood of observed labels = -36734.82152213\n",
      "iteration   9: log likelihood of observed labels = -36729.08669961\n",
      "iteration  10: log likelihood of observed labels = -36723.35786366\n",
      "iteration  11: log likelihood of observed labels = -36717.63499744\n",
      "iteration  12: log likelihood of observed labels = -36711.91808422\n",
      "iteration  13: log likelihood of observed labels = -36706.20710739\n",
      "iteration  14: log likelihood of observed labels = -36700.50205049\n",
      "iteration  15: log likelihood of observed labels = -36694.80289716\n",
      "iteration  20: log likelihood of observed labels = -36666.39512033\n",
      "iteration  30: log likelihood of observed labels = -36610.01327118\n",
      "iteration  40: log likelihood of observed labels = -36554.19728365\n",
      "iteration  50: log likelihood of observed labels = -36498.93316099\n",
      "iteration  60: log likelihood of observed labels = -36444.20783914\n",
      "iteration  70: log likelihood of observed labels = -36390.00909449\n",
      "iteration  80: log likelihood of observed labels = -36336.32546144\n",
      "iteration  90: log likelihood of observed labels = -36283.14615871\n",
      "iteration 100: log likelihood of observed labels = -36230.46102347\n",
      "iteration 200: log likelihood of observed labels = -35728.89418769\n",
      "iteration 300: log likelihood of observed labels = -35268.51212683\n"
     ]
    }
   ],
   "source": [
    "coefficients = logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25126"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(predict_probability(feature_matrix,coefficients) >= 0.5,dtype=int).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.asarray(predict_probability(feature_matrix, coefficients) >= 0.5,dtype=int)\n",
    "predictions_p = np.array(predictions[0], dtype = int)\n",
    "sentiment_p = np.asarray(sentiment, dtype = int)[0]\n",
    "def my_mapper(lst):\n",
    "    for x in range(len(lst)):\n",
    "        if lst[x] == -1:\n",
    "            lst[x] = 0\n",
    "    return lst\n",
    "sentiment_pv = my_mapper(sentiment_p)\n",
    "print(sentiment_pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 0 1 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "0.7518653904130238\n"
     ]
    }
   ],
   "source": [
    "correct = np.asarray(predictions_p == sentiment_pv, dtype=int).sum()\n",
    "print(predictions_p)\n",
    "print(sentiment_pv)\n",
    "measure_metrics = correct / len(predictions_p)\n",
    "print(measure_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = list(coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients)]\n",
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.0665460841704577),\n",
       " ('love', 0.06589076292212327),\n",
       " ('easy', 0.0647945868025784),\n",
       " ('little', 0.04543562630842138),\n",
       " ('loves', 0.04497640139490604),\n",
       " ('well', 0.030135001092107074),\n",
       " ('perfect', 0.02973993710496846),\n",
       " ('old', 0.02007754103477538),\n",
       " ('nice', 0.018408707995268992),\n",
       " ('daughter', 0.017703199905701694)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('monitor', -0.024482100545891717),\n",
       " ('return', -0.026592778462247283),\n",
       " ('back', -0.02774269723066133),\n",
       " ('get', -0.028711552980192585),\n",
       " ('disappointed', -0.028978976142317068),\n",
       " ('even', -0.030051249236035808),\n",
       " ('work', -0.03306951529475273),\n",
       " ('money', -0.03898203728648711),\n",
       " ('product', -0.041511033392108904),\n",
       " ('would', -0.053860148445203114)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Part 2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_processed = np.transpose(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53072, 194)\n",
      "(53072, 1)\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix.shape)\n",
    "print(sentiment_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, validation_data = products.random_split(.8, seed=2)\n",
    "train_data, validation_data, train_output, validation_output = train_test_split(np.array(feature_matrix), np.array(sentiment_processed), test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_probability(train_data, new_init_weight)\n",
    "indi = (train_output==+1).reshape((1,-1))[0]\n",
    "err = indi - pred\n",
    "derv = feature_derivative_with_L2(err, train_data[:, 1], new_init_weight[1], l2_penalties[1], False)\n",
    "lpp = compute_log_likelihood_with_L2(train_data, train_output.reshape((1,-1))[0], new_init_weight, l2_penalties[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "    \n",
    "    # Compute the dot product of errors and feature\n",
    "    ## YOUR CODE HERE\n",
    "    derivative = np.dot(errors, feature)\n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant: \n",
    "        ## YOUR CODE HERE\n",
    "        derivative -= 2 * l2_penalty * coefficient\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    ev = np.multiply((indicator-1.), scores)\n",
    "    nv = np.log(1. + np.exp(-scores))\n",
    "    l2 = np.multiply(l2_penalty, np.sum(coefficients[1:]**2))\n",
    "    \n",
    "    lp = np.sum(np.subtract(np.subtract(ev, nv), l2))\n",
    "    \n",
    "    #lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    sentiment = sentiment.reshape((1,-1))[0]\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        \n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative_with_L2(errors, feature_matrix[:, j], coefficients[j], l2_penalty, is_intercept)\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j] += step_size * derivative\n",
    "            \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            \n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_step_size = 5e-6\n",
    "new_max_iter = 501\n",
    "l2_penalties = [0, 4, 10, 100, 1e3, 1e5]\n",
    "new_init_weight = np.zeros(train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29244.82697959\n",
      "iteration   1: log likelihood of observed labels = -29068.30359513\n",
      "iteration   2: log likelihood of observed labels = -28898.56887514\n",
      "iteration   3: log likelihood of observed labels = -28735.03180945\n",
      "iteration   4: log likelihood of observed labels = -28577.23777802\n",
      "iteration   5: log likelihood of observed labels = -28424.81887564\n",
      "iteration   6: log likelihood of observed labels = -28277.46424082\n",
      "iteration   7: log likelihood of observed labels = -28134.90216992\n",
      "iteration   8: log likelihood of observed labels = -27996.88918670\n",
      "iteration   9: log likelihood of observed labels = -27863.20324128\n",
      "iteration  10: log likelihood of observed labels = -27733.63937739\n",
      "iteration  11: log likelihood of observed labels = -27608.00688484\n",
      "iteration  12: log likelihood of observed labels = -27486.12735202\n",
      "iteration  13: log likelihood of observed labels = -27367.83326901\n",
      "iteration  14: log likelihood of observed labels = -27252.96697285\n",
      "iteration  15: log likelihood of observed labels = -27141.37980989\n",
      "iteration  20: log likelihood of observed labels = -26627.97847668\n",
      "iteration  30: log likelihood of observed labels = -25781.24963952\n",
      "iteration  40: log likelihood of observed labels = -25110.24640218\n",
      "iteration  50: log likelihood of observed labels = -24563.82024533\n",
      "iteration  60: log likelihood of observed labels = -24108.99476117\n",
      "iteration  70: log likelihood of observed labels = -23723.60749538\n",
      "iteration  80: log likelihood of observed labels = -23392.21903204\n",
      "iteration  90: log likelihood of observed labels = -23103.73601778\n",
      "iteration 100: log likelihood of observed labels = -22849.97305146\n",
      "iteration 200: log likelihood of observed labels = -21342.18863001\n",
      "iteration 300: log likelihood of observed labels = -20631.77609487\n",
      "iteration 400: log likelihood of observed labels = -20213.75546520\n",
      "iteration 500: log likelihood of observed labels = -19938.34961891\n"
     ]
    }
   ],
   "source": [
    "coefficients_0_penalty = logistic_regression_with_L2(train_data, train_output, initial_coefficients, new_step_size, l2_penalties[0], new_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29402.88901766\n",
      "iteration   1: log likelihood of observed labels = -29686.95964644\n",
      "iteration   2: log likelihood of observed labels = -30262.05447085\n",
      "iteration   3: log likelihood of observed labels = -31111.41488188\n",
      "iteration   4: log likelihood of observed labels = -32219.97629854\n",
      "iteration   5: log likelihood of observed labels = -33573.96154239\n",
      "iteration   6: log likelihood of observed labels = -35160.62290537\n",
      "iteration   7: log likelihood of observed labels = -36968.07383400\n",
      "iteration   8: log likelihood of observed labels = -38985.17499716\n",
      "iteration   9: log likelihood of observed labels = -41201.45353457\n",
      "iteration  10: log likelihood of observed labels = -43607.04265992\n",
      "iteration  11: log likelihood of observed labels = -46192.63388105\n",
      "iteration  12: log likelihood of observed labels = -48949.43720919\n",
      "iteration  13: log likelihood of observed labels = -51869.14661896\n",
      "iteration  14: log likelihood of observed labels = -54943.90915325\n",
      "iteration  15: log likelihood of observed labels = -58166.29673409\n",
      "iteration  20: log likelihood of observed labels = -76259.23416575\n",
      "iteration  30: log likelihood of observed labels = -120205.18133764\n",
      "iteration  40: log likelihood of observed labels = -171250.68038771\n",
      "iteration  50: log likelihood of observed labels = -226821.42357791\n",
      "iteration  60: log likelihood of observed labels = -285273.28989298\n",
      "iteration  70: log likelihood of observed labels = -345520.37601008\n",
      "iteration  80: log likelihood of observed labels = -406825.43871565\n",
      "iteration  90: log likelihood of observed labels = -468676.69556060\n",
      "iteration 100: log likelihood of observed labels = -530712.56349621\n",
      "iteration 200: log likelihood of observed labels = -1128010.14402734\n"
     ]
    }
   ],
   "source": [
    "coefficients_4_penalty = logistic_regression_with_L2(train_data, train_output, initial_coefficients, new_step_size, l2_penalties[1], new_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_10_penalty = logistic_regression_with_L2(train_data, train_output, initial_coefficients, new_step_size, l2_penalties[2], new_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_1e2_penalty = logistic_regression_with_L2(train_data, train_output, initial_coefficients, new_step_size, l2_penalties[3], new_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_1e3_penalty = logistic_regression_with_L2(train_data, train_output, initial_coefficients, new_step_size, l2_penalties[4], new_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients_1e5_penalty = logistic_regression_with_L2(train_data, train_output, initial_coefficients, new_step_size, l2_penalties[5], new_max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
