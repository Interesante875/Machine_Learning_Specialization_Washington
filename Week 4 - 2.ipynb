{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import spdiags\n",
    "from scipy.stats import multivariate_normal\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import mlab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = pd.read_csv('Week_4/people_wiki.csv').head(5000)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    data = loader['data']\n",
    "    indices = loader['indices']\n",
    "    indptr = loader['indptr']\n",
    "    shape = loader['shape']\n",
    "    \n",
    "    return csr_matrix( (data, indices, indptr), shape)\n",
    "\n",
    "tf_idf = load_sparse_csr('Week_4/4_tf_idf.npz')  \n",
    "with open('Week_4/4_map_index_to_word.json') as f:\n",
    "    map_index_to_word = pd.Series(json.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diag(array):\n",
    "    n = len(array)\n",
    "    return spdiags(array, 0, n, n)\n",
    "\n",
    "def logpdf_diagonal_gaussian(x, mean, cov):\n",
    "    '''\n",
    "    Compute logpdf of a multivariate Gaussian distribution with diagonal covariance at a given point x.\n",
    "    A multivariate Gaussian distribution with a diagonal covariance is equivalent\n",
    "    to a collection of independent Gaussian random variables.\n",
    "\n",
    "    x should be a sparse matrix. The logpdf will be computed for each row of x.\n",
    "    mean and cov should be given as 1D numpy arrays\n",
    "    mean[i] : mean of i-th variable\n",
    "    cov[i] : variance of i-th variable'''\n",
    "\n",
    "    n = x.shape[0]\n",
    "    dim = x.shape[1]\n",
    "    assert(dim == len(mean) and dim == len(cov))\n",
    "\n",
    "    # multiply each i-th column of x by (1/(2*sigma_i)), where sigma_i is sqrt of variance of i-th variable.\n",
    "    scaled_x = x.dot(diag(1./(2*np.sqrt(cov))) )\n",
    "    # multiply each i-th entry of mean by (1/(2*sigma_i))\n",
    "    scaled_mean = mean/(2*np.sqrt(cov))\n",
    "\n",
    "    # sum of pairwise squared Eulidean distances gives SUM[(x_i - mean_i)^2/(2*sigma_i^2)]\n",
    "    return -np.sum(np.log(np.sqrt(2*np.pi*cov))) - pairwise_distances(scaled_x, [scaled_mean], 'euclidean').flatten()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x, axis):\n",
    "    '''Compute the log of a sum of exponentials'''\n",
    "    x_max = np.max(x, axis=axis)\n",
    "    if axis == 1:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max[:,np.newaxis]), axis=1) )\n",
    "    else:\n",
    "        return x_max + np.log( np.sum(np.exp(x-x_max), axis=0) )\n",
    "\n",
    "def EM_for_high_dimension(data, means, covs, weights, cov_smoothing=1e-5, maxiter=int(1e3), thresh=1e-4, verbose=False):\n",
    "    # cov_smoothing: specifies the default variance assigned to absent features in a cluster.\n",
    "    #                If we were to assign zero variances to absent features, we would be overconfient,\n",
    "    #                as we hastily conclude that those featurese would NEVER appear in the cluster.\n",
    "    #                We'd like to leave a little bit of possibility for absent features to show up later.\n",
    "    n = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "    mu = deepcopy(means)\n",
    "    Sigma = deepcopy(covs)\n",
    "    K = len(mu)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    ll = None\n",
    "    ll_trace = []\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        # E-step: compute responsibilities\n",
    "        logresp = np.zeros((n,K))\n",
    "        for k in range(K):\n",
    "            logresp[:,k] = np.log(weights[k]) + logpdf_diagonal_gaussian(data, mu[k], Sigma[k])\n",
    "        ll_new = np.sum(log_sum_exp(logresp, axis=1))\n",
    "        if verbose:\n",
    "            print(ll_new)\n",
    "        logresp -= np.vstack(log_sum_exp(logresp, axis=1))\n",
    "        resp = np.exp(logresp)\n",
    "        counts = np.sum(resp, axis=0)\n",
    "\n",
    "        # M-step: update weights, means, covariances\n",
    "        weights = counts / np.sum(counts)\n",
    "        for k in range(K):\n",
    "            mu[k] = (diag(resp[:,k]).dot(data)).sum(axis=0)/counts[k]\n",
    "            mu[k] = mu[k].A1\n",
    "\n",
    "            Sigma[k] = diag(resp[:,k]).dot( data.multiply(data)-2*data.dot(diag(mu[k])) ).sum(axis=0) \\\n",
    "                       + (mu[k]**2)*counts[k]\n",
    "            Sigma[k] = Sigma[k].A1 / counts[k] + cov_smoothing*np.ones(dim)\n",
    "\n",
    "        # check for convergence in log-likelihood\n",
    "        ll_trace.append(ll_new)\n",
    "        if ll is not None and (ll_new-ll) < thresh and ll_new > -np.inf:\n",
    "            ll = ll_new\n",
    "            break\n",
    "        else:\n",
    "            ll = ll_new\n",
    "\n",
    "    out = {'weights':weights,'means':mu,'covs':Sigma,'loglik':ll_trace,'resp':resp}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(5)\n",
    "num_clusters = 25\n",
    "\n",
    "# Use scikit-learn's k-means to simplify workflow\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=-1)\n",
    "kmeans_model.fit(tf_idf)\n",
    "centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "\n",
    "means = [centroid for centroid in centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = tf_idf.shape[0]\n",
    "weights = []\n",
    "for i in range(num_clusters):\n",
    "    # Compute the number of data points assigned to cluster i:\n",
    "    num_assigned =  np.sum(cluster_assignment==i) # YOUR CODE HERE\n",
    "    w = float(num_assigned) / num_docs\n",
    "    weights.append(w) ##Initializing covariances.** To initialize our covariance parameters, we compute $\\hat{\\sigma}_{k, j}^2 = \\sum_{i=1}^{N}(x_{i,j} - \\hat{\\mu}_{k, j})^2$ for each feature $j$.  For features with really tiny variances, we assign 1e-8 instead to prevent numerical instability. We do this computation in a vectorized fashion in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = []\n",
    "for i in range(num_clusters):\n",
    "    member_rows = tf_idf[cluster_assignment==i]\n",
    "    cov = (member_rows.multiply(member_rows) - 2*member_rows.dot(diag(means[i]))).sum(axis=0).A1 / member_rows.shape[0] \\\n",
    "          + means[i]**2\n",
    "    cov[cov < 1e-8] = 1e-8\n",
    "    covs.append(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2676617808.9448195, 3418296378.1485095, 3418296378.1485095]\n"
     ]
    }
   ],
   "source": [
    "out = EM_for_high_dimension(tf_idf, means, covs, weights, cov_smoothing=1e-10)\n",
    "print (out['loglik']) # print history of log-likelihood over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "def visualize_EM_clusters(tf_idf, means, covs, map_index_to_word):\n",
    "    print('')\n",
    "    print('==========================================================')\n",
    "\n",
    "    num_clusters = len(means)\n",
    "    for c in range(num_clusters):\n",
    "        print('Cluster {0:d}: Largest mean parameters in cluster '.format(c))\n",
    "        print('\\n{0: <12}{1: <12}{2: <12}'.format('Word', 'Mean', 'Variance'))\n",
    "        \n",
    "        # The k'th element of sorted_word_ids should be the index of the word \n",
    "        # that has the k'th-largest value in the cluster mean. Hint: Use np.argsort().\n",
    "        sorted_word_ids = np.argsort(-means[c])   # YOUR CODE HERE\n",
    "\n",
    "        for i in sorted_word_ids[:5]:\n",
    "            print ('{0: <12}{1:<10.2e}{2:10.2e}'.format(map_index_to_word.index[map_index_to_word==i][0], \n",
    "                                                       means[c][i],\n",
    "                                                       covs[c][i]))\n",
    "        print ('\\n=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "polye       8.52e+01    1.00e-10\n",
      "minister    3.36e+01    1.00e-10\n",
      "prime       2.81e+01    1.00e-10\n",
      "party       2.80e+01    1.00e-10\n",
      "cabinet     1.96e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "kirk        1.37e+02    1.00e-10\n",
      "assists     8.44e+01    1.00e-10\n",
      "rebounds    8.14e+01    1.00e-10\n",
      "steals      8.08e+01    1.00e-10\n",
      "averaged    6.36e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "film        3.79e+01    1.77e+02\n",
      "festival    2.87e+01    2.45e+02\n",
      "geographic  2.44e+01    5.96e+02\n",
      "video       2.01e+01    2.90e+02\n",
      "1991        1.87e+01    3.49e+02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "plaintiff   6.66e+02    1.00e-10\n",
      "plaintiffs  3.09e+02    1.00e-10\n",
      "student     2.98e+02    1.00e-10\n",
      "morrison    2.27e+02    1.00e-10\n",
      "evidence    2.23e+02    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "islamic     3.01e+01    1.31e+03\n",
      "chun        2.84e+01    4.04e+03\n",
      "elkhosht    2.56e+01    3.26e+03\n",
      "ge          2.07e+01    2.15e+03\n",
      "rasheed     1.70e+01    1.45e+03\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "university  1.31e+00    3.13e+00\n",
      "research    1.12e+00    7.12e+00\n",
      "art         1.07e+00    1.38e+01\n",
      "he          9.99e-01    5.34e-01\n",
      "book        9.66e-01    4.74e+00\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "air         5.33e+00    3.03e+02\n",
      "law         4.55e+00    1.39e+02\n",
      "business    4.50e+00    3.75e+01\n",
      "health      4.33e+00    2.86e+02\n",
      "energy      4.18e+00    6.22e+01\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "party       2.66e+00    1.52e+01\n",
      "election    2.60e+00    1.70e+01\n",
      "minister    2.50e+00    2.95e+01\n",
      "she         2.26e+00    2.96e+01\n",
      "law         2.01e+00    1.70e+01\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "flyweight   8.98e+01    1.00e-10\n",
      "gonzalez    8.70e+01    1.00e-10\n",
      "round       6.20e+01    1.00e-10\n",
      "title       4.83e+01    1.00e-10\n",
      "champion    4.83e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "rabbi       1.72e+01    4.36e+02\n",
      "bible       7.29e+00    1.54e+02\n",
      "talit       6.39e+00    6.12e+02\n",
      "anglican    6.33e+00    2.57e+02\n",
      "church      6.32e+00    1.26e+02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         3.84e+00    3.71e+01\n",
      "music       3.33e+00    2.52e+01\n",
      "her         3.10e+00    2.67e+01\n",
      "album       2.33e+00    2.56e+01\n",
      "film        2.27e+00    2.43e+01\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "technology  3.23e+01    1.00e-10\n",
      "innovation  2.65e+01    1.00e-10\n",
      "hkust       2.56e+01    1.00e-10\n",
      "kong        2.48e+01    1.00e-10\n",
      "hong        2.45e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "kelly       7.36e+01    1.00e-10\n",
      "semi        3.73e+01    1.00e-10\n",
      "uefa        3.72e+01    1.00e-10\n",
      "ireland     3.60e+01    1.00e-10\n",
      "aggregate   3.29e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "yanomami    2.04e+02    1.00e-10\n",
      "davi        1.11e+02    1.00e-10\n",
      "kopenawa    6.81e+01    1.00e-10\n",
      "garimpeiros 4.26e+01    1.00e-10\n",
      "diseases    3.87e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "symphony    1.53e+02    1.00e-10\n",
      "worby       1.19e+02    1.00e-10\n",
      "conductor   6.02e+01    1.00e-10\n",
      "pasadena    4.65e+01    1.00e-10\n",
      "wheeling    4.45e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "myrna       7.67e+01    1.00e-10\n",
      "lorrie      5.48e+01    1.00e-10\n",
      "she         3.11e+01    1.00e-10\n",
      "country     2.93e+01    1.00e-10\n",
      "cbc         2.53e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "championships7.48e+00    5.12e+01\n",
      "she         5.57e+00    6.90e+01\n",
      "m           4.86e+00    8.26e+01\n",
      "olympics    4.45e+00    2.02e+01\n",
      "medal       4.37e+00    2.22e+01\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "backstroke  5.23e+01    1.47e+02\n",
      "m           1.82e+01    2.45e+01\n",
      "tarabrin    1.42e+01    4.03e+02\n",
      "erke        1.42e+01    4.03e+02\n",
      "kiss        1.16e+01    2.71e+02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "kotik       1.87e+02    1.00e-10\n",
      "sem         5.48e+01    1.00e-10\n",
      "kotiks      3.41e+01    1.00e-10\n",
      "prague      3.11e+01    1.00e-10\n",
      "orchestra   3.05e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "engineering 1.55e+01    1.05e+02\n",
      "aircraft    7.90e+00    4.24e+02\n",
      "research    7.60e+00    4.47e+01\n",
      "wayne       6.42e+00    5.23e+02\n",
      "fossella    6.25e+00    5.46e+02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "haar        1.10e+02    1.00e-10\n",
      "vander      1.02e+02    1.00e-10\n",
      "essendon    5.23e+01    1.00e-10\n",
      "haars       1.70e+01    1.00e-10\n",
      "essendons   1.70e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "league      4.10e+00    2.28e+01\n",
      "season      3.67e+00    1.48e+01\n",
      "football    2.87e+00    2.31e+01\n",
      "played      2.65e+00    6.17e+00\n",
      "team        2.60e+00    9.01e+00\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "hilliard    1.72e+02    1.00e-10\n",
      "interlochen 4.60e+01    1.00e-10\n",
      "hilliards   4.26e+01    1.00e-10\n",
      "music       3.74e+01    1.00e-10\n",
      "cornell     3.71e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "she         1.36e+01    2.64e+02\n",
      "india       8.33e+00    7.38e+01\n",
      "indian      6.57e+00    4.01e+01\n",
      "her         5.97e+00    5.95e+01\n",
      "film        5.48e+00    1.16e+02\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "cave        7.47e+01    1.00e-10\n",
      "diving      4.83e+01    1.00e-10\n",
      "farr        3.41e+01    1.00e-10\n",
      "caving      2.35e+01    1.00e-10\n",
      "underwater  1.43e+01    1.00e-10\n",
      "\n",
      "=====================================================Quiz Question. Select all the topics that have a cluster in the model created above. [multiple choice]====\n"
     ]
    }
   ],
   "source": [
    "'''By EM'''\n",
    "visualize_EM_clusters(tf_idf, out['means'], out['covs'], map_index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "num_clusters = len(means)\n",
    "num_docs, num_words = tf_idf.shape\n",
    "\n",
    "random_means = []\n",
    "random_covs = []\n",
    "random_weights = []\n",
    "\n",
    "for k in range(num_clusters):\n",
    "    \n",
    "    # Create a numpy array of length num_words with random normally distributed values.\n",
    "    # Use the standard univariate normal distribution (mean 0, variance 1).\n",
    "    # YOUR CODE HERE\n",
    "    mean = np.random.normal(0, 1, num_words)\n",
    "    \n",
    "    # Create a numpy array of length num_words with random values uniformly distributed between 1 and 5.\n",
    "    # YOUR CODE HERE\n",
    "    cov = np.random.uniform(1,5, num_words)\n",
    "\n",
    "    # Initially give each cluster equal weight.\n",
    "    # YOUR CODE HERE\n",
    "    weight = 1.0/num_clusters\n",
    "    \n",
    "    random_means.append(mean)\n",
    "    random_covs.append(cov)\n",
    "    random_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-768078342.4726903, 1647324823.7563128, 1692184172.410275, 1692184172.410275]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_random_init = EM_for_high_dimension(tf_idf, random_means, random_covs, random_weights, cov_smoothing=1e-5)\n",
    "out_random_init['loglik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
